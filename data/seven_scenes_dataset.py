"""
Name: seven_scenes_dataset.py
Description: Functions used to load the input data and for building the graphs that are fed to PoserNet.
             The function that builds one graph is: get_data_for_one_graph().
-----
Author: Matteo Taiana.
Licence: MIT. Copyright 2022, PAVIS, Istituto Italiano di Tecnologia.
Acknowledgment:
This project has received funding from the European Union's Horizon 2020
research and innovation programme under grant agreement No 870743.
"""
# Path hack used to import scripts from sibling directories.
import sys, os
sys.path.insert(0, os.path.abspath('..'))

# Generic imports.
from pathlib import Path
import numpy as np
import pickle
import torch
import copy
from pytransform3d import rotations as rot
import json
from joblib import Parallel, delayed
import multiprocessing

from data.my_graph import MyGraph
from utils import pose_algebra
from core.embeddings_definition import node_features_size, det_features_size, edge_features_size



def generate_random_quaternion():
    # Formula taken from: http://planning.cs.uiuc.edu/node198.html
    u1 = torch.rand(1)
    u2 = torch.rand(1)
    u3 = torch.rand(1)
    pi = torch.tensor(np.pi)
    q = torch.tensor((torch.sqrt(1-u1) * torch.sin(2*pi*u2),
                      torch.sqrt(1-u1) * torch.cos(2*pi*u2),
                      torch.sqrt(u1)   * torch.sin(2*pi*u3),
                      torch.sqrt(u1)   * torch.cos(2*pi*u3)), device='cpu')
    return q



def generate_random_direction():
    t = (torch.rand(1, 3)-0.5) * 2
    t = t/torch.norm(t)
    return t



def corrupt_relative_pose(rel_pose_quaternion, translation_noise_limit=4, rotation_noise_limit_deg=30):
    # This function first translates the pose to the origin, then applies the corrupting rotation, then translates the pose
    # back to the original location, and finally applies the corrupting translation.
    # PARAMETERS:
    # translation_noise_limit: the noisy relative camera translation is generated by adding uniformly sampled noise between -this, +this for each axis.
    # rotation_noise_limit_deg: The noisy relative camera orientation is generated sampling uniformly between -this, +this,
    # and rotating about a random axis (each component is selected randomly).

    rotation_noise_limit_rad = np.deg2rad(rotation_noise_limit_deg)

    # 0. Transform the pose quaternion into a transformation matrix.
    pose_quat_np = rel_pose_quaternion.cpu().numpy()
    R = rot.matrix_from_quaternion(pose_quat_np[0:4])
    # Create the full [4x4] transformation matrix.
    Rt = np.vstack((np.hstack((R, pose_quat_np[4:7].reshape(3, 1))), np.array((0, 0, 0, 1))))

    # 1. Remove translation.
    inverse_translation = np.eye(4)
    inverse_translation[0:3, 3] = -Rt[0:3, 3]
    temp = np.dot(inverse_translation, Rt)

    # 2. Apply rotation noise.
    rot_noise_axis_angle = rot.random_axis_angle()  # This picks the direction of the rotation axis randomly.
    rot_noise_axis_angle[3] = np.random.uniform(-rotation_noise_limit_rad, rotation_noise_limit_rad)  # This sets the value for the amount of rotation.
    rot_noise_mat = rot.matrix_from_axis_angle(rot_noise_axis_angle)
    temp[0:3, 0:3] = np.dot(rot_noise_mat, temp[0:3, 0:3])

    # 3. Compute original translation as seen from the new orientation.
    original_translation_vector = Rt[0:3, 3]
    original_translation_vector = np.dot(rot_noise_mat, original_translation_vector)
    original_translation = np.eye(4)
    original_translation[0:3, 3] = original_translation_vector

    # 4. Apply original translation.
    temp = np.dot(original_translation, temp)

    # 5. Verify that the location encoded in "temp" is the same as that encoded in "Rt".
    assert (np.linalg.inv(Rt)[:, 3] == np.linalg.inv(temp)[:, 3]).all

    # 6. Apply translation noise.
    translation_noise = np.random.uniform(-translation_noise_limit, translation_noise_limit, size=3)  # Generate 3 values in [-lim, +lim] with uniform probability.
    temp[0:3, 3] += translation_noise

    # 7. Convert the transformation matrix back to a quaternion.
    corrupted_pose_quaternion = np.zeros(7)
    corrupted_pose_quaternion[0:4] = rot.quaternion_from_matrix(temp[0:3, 0:3])
    corrupted_pose_quaternion[4:7] = temp[0:3, 3]

    return torch.tensor(corrupted_pose_quaternion)



def corrupt_training_set(data_list, translation_noise_limit=0.1, rotation_noise_limit_deg=6):
    print('Corrupting training data.')
    corrupted_data_list = copy.deepcopy(data_list)
    for graph in corrupted_data_list:
        for edge in graph.edge_attr:
            edge[0:7] = corrupt_relative_pose(edge[0:7], translation_noise_limit=translation_noise_limit, rotation_noise_limit_deg=rotation_noise_limit_deg)
    return corrupted_data_list



def get_data_for_one_graph(input_graph, graph_id, type_of_input_estimates, this_is_training=True, left_out_scene_id=None):
    scene_names = ['chess',       # 0
                   'fire',        # 1
                   'heads',       # 2
                   'office',      # 3
                   'pumpkin',     # 4
                   'redkitchen',  # 5
                   'stairs']      # 6
    if not isinstance(left_out_scene_id, type(None)): # If the scene to be left out was selected
        id_for_first_image = list(input_graph['cameras'].keys())[0]
        this_graph_belongs_to_the_left_out_scene = scene_names[left_out_scene_id] in input_graph['cameras'][id_for_first_image]['image_path']
        if (this_is_training and this_graph_belongs_to_the_left_out_scene) or \
            (not(this_is_training) and not(this_graph_belongs_to_the_left_out_scene)):
            # print('Skipping graph that contains {:s}.'.format(input_graph['cameras'][id_for_first_image]['image_path']))
            return []  # Return an empty graph which will be filtered out.

    # Algorithm
    # 0. Create empty data structures.
    # 1. Fill nodes (x), det_features, x_det_ptr
    #    For each node:
    #       Fill row of x for current node.
    #       Make list of detections of current node.
    #       Add detection embeddings in det_features.
    #       Add pointers from x_det_ptr to det_features.
    # 2. Fill edge connectivity (edge_index), initial edge embeddings (edge_attr) and GT edge embeddings (y).
    #    For all possible edges (take only one direction into account):
    #       If covisibility(source, dest) == 1:
    #           Add edge to edge_index.
    #           Compute GT relative pose transformation between source and dest (y).
    #           Compute initial estimate for relative pose transformation (edge_attr) by corrupting the GT values (y).
    # 3. Compute the indices that will be used to select the data that will be the input for the MLP's.
    #       Create the indices for the input of the edge MLP.

    with torch.no_grad():  # No need to compute gradients for the initialisation operations.
        if len(input_graph['cameras']) == len(input_graph['detections']['covisibility']):  # Graphs that do not satisfy this condition are not well-formed, so they are skipped.

            # Normalisation parameters
            standard_image_width  = 640 + 4   # Piked by hand. I added the +4 and + 3 so that the input value for the neurons would not be 0.
            standard_image_height = 480 + 3   # Piked by hand, for a 3:4 ratio with width.
            standard_focal_length = 585       # Piked by hand.
            standard_radial_distortion = 0.05  # Piked by hand.


            #######################################################
            # 0. Set up: create variables to be filled with data. #
            #######################################################
            # Create det_obj_ids and det_features.
            # They list all the detections of the current graph. The rows of the two structures refer to the same object.
            # The structures are grown on the go, this is probably inefficient.
            # det_obj_id encodes the ID of the physical object the detection refers to.
            # det_features encodes the features of the corresponding detection.
            det_features = torch.zeros(0, det_features_size, dtype=torch.float, device='cpu')

            n_nodes = len(input_graph['cameras'])

            # Create x_det_ptr.
            # It is a data structure specific to our problem, it associates a node with the information on
            # the detections present on the corresponding image. It contains indices (pointers) to detections stored in the
            # "detections" data structure.
            # Initialise the indices with the invalid value of -1.
            max_detections_per_node = 40  # TODO: expose variable.
            x_det_ptr = -torch.ones(n_nodes, max_detections_per_node, dtype=torch.int, device='cpu')

            # Create x: the feature vectors associated with the nodes (one node per row).
            # Each node feature vector consists of: image width, height, focal length and first radial distortion parameter, somehow normalised.
            # The shape of x is: [n_nodes x node_features_size].
            x = torch.zeros(n_nodes, node_features_size, dtype=torch.float, device='cpu')

            # Create gt_absolute_poses: [4x(4*n_nodes)] Stores the GT absolute pose for each camera.
            # This data is not used by PoseNet, it is used by the Rotation Average algorithm to measure its performance.
            gt_absolute_poses = torch.zeros(4, 4*n_nodes, dtype=torch.float, device='cpu')


            # Create edge_index and edge_attr: set the connectivity for the graph and set the input values for the
            # edge features. Edge features contain relative camera poses.
            # Connections are directional: 1-0 is different from 0-1. Still, at this point we use only one edge to
            # connect a pair of nodes. The symmetric edges will be created later.
            # The graph does not have to be fully connected.
            n_edges = int((n_nodes * (n_nodes - 1)) / 2)

            # edge_index: connectivity information.
            edge_index = torch.zeros(2, n_edges, dtype=torch.long, device='cpu')

            # Feature vectors associated with the edges: filled with initial estimates (or corrupted GT) for relative camera poses.
            edge_attr = torch.zeros(n_edges, edge_features_size, dtype=torch.float, device='cpu')

            # y: the desired/GT values for the edge features. The GT relative camera poses.
            y = torch.zeros(n_edges, edge_features_size, dtype=torch.float, device='cpu')

            n_dets = torch.zeros(1, 1, dtype=torch.int, device='cpu')  # Total number of detections associated with the current graph.


            #################################################
            # 1. Fill the nodes and detections information: #
            # x, det_features, x_det_ptr, n_dets.           #
            #################################################
            # image_index == node_id
            for image_index, image_id in enumerate(input_graph['cameras']):
                # Prepare the information to be written as encoding for the current node.
                current_image = input_graph['cameras'][str(image_id)]
                image_width = 640
                norm_image_width = (image_width - standard_image_width) / standard_image_width
                image_height = 480
                norm_image_height = (image_height - standard_image_height) / standard_image_height
                focal_length = 590
                norm_focal_length = (focal_length - standard_focal_length) / standard_focal_length
                radial_distortion = 0.06
                norm_radial_distortion = (radial_distortion - standard_radial_distortion) / standard_radial_distortion

                # Write the encoding of the current node in x.
                x[image_index, :] = torch.tensor([norm_image_width, norm_image_height, norm_focal_length, norm_radial_distortion])

                # Save the GT absolute pose for the current camera (information not used by PoserNet, it is passed on to the Motion Averaging algorithm for evaluation).
                gt_absolute_poses[:, image_index*4:(image_index+1)*4] = torch.tensor(input_graph['cameras'][image_id]['camera_to_world'], device='cpu')

                # Create a list with the detections of the current node and store it in the JSON data structure.
                # The order will be the same as the one used in x_det_ptr.
                detections = []
                for dest_image_id in current_image['matches']:
                    for match in current_image['matches'][dest_image_id]:
                        if not match['bb_i'] in detections:
                            detections.append(match['bb_i'])
                current_image['detections'] = detections

                # Prepare and write the encodings of the detections in det_features, connect the nodes with their detections
                # by writing information in x_det_ptr.
                for detection_index, detection in enumerate(current_image['detections']):
                    if detection_index < max_detections_per_node:
                        # Compute embeddings for current detection
                        min_x, min_y, max_x, max_y = detection
                        det_x_centre = (((max_x + min_x) / 2) - (image_width / 2))  / (image_width / 2)   # Detection horizontal centre -> [-1..1]
                        det_y_centre = (((max_y + min_y) / 2) - (image_height / 2)) / (image_height / 2)  # Detection vertical centre -> [-1..1]
                        det_width    = ((max_x - min_x)       - (image_width / 2))  / (image_width / 2)   # Detection width -> [-1..1]
                        det_height   = ((max_y - min_y)       - (image_height / 2)) / (image_height / 2)  # Detection height -> [-1..1]
                        current_det_features = torch.tensor([det_x_centre, det_y_centre, det_width, det_height], dtype=torch.float, device='cpu').reshape(1, det_features_size)

                        if det_x_centre < -1 or det_x_centre > 1 or det_y_centre < -1 or det_y_centre > 1 or det_width < -1 or det_width > 1 or det_height < -1 or det_height > 1:
                            print('something is wrong with the BB computation.')

                        # Add the data relative to the current detection to the last row to 'detections'.
                        det_features = torch.cat([det_features, current_det_features], dim=0)

                        # Make the right cell in x_det_ptr point to the currently last row of detections.
                        x_det_ptr[image_index, detection_index] = n_dets

                        n_dets += 1
                    else:
                        print('Limiting the number of detections on one image.')



            ##################################
            # 2. Fill the edges information: #
            # edge_attr, edge_index, y.      #
            ##################################
            edge_id = 0
            # images = sequence['image_info']['images']
            covis = input_graph['detections']['covisibility']  # It is a list of lists, value = 1.0 means that the two images see enough matched objects.
            for source_node_index, source_node_id in enumerate(input_graph['cameras']):
                for dest_node_index, dest_node_id in enumerate(input_graph['cameras']):
                    if dest_node_index > source_node_index:
                        if covis[source_node_index][dest_node_index] > 0.5:  # The value is 1 when the two cameras see enough common objects.

                            # Create a different graph depending on which source was selected
                            if type_of_input_estimates == 0:  # 0: Corrupted GT, available for all edges.
                                # Corrupt the GT values of the relative transformation and store the result on the edge, as input values.
                                edge_attr[edge_id, 0:7] = corrupt_relative_pose(y[edge_id, :], translation_noise_limit=0.5, rotation_noise_limit_deg=30)

                                # Create the edge (add it to the data structure).
                                edge_index[:, edge_id] = torch.tensor([source_node_index, dest_node_index])

                                # Get the absolute camera poses.
                                gt_source_T = input_graph['cameras'][str(source_node_id)]['camera_to_world']  # Extrinsics. This is the transformation that maps a 3D point from the camera reference frame to the world reference frame.
                                gt_dest_T   = input_graph['cameras'][str(dest_node_id)]['camera_to_world']    # Extrinsics. This is the transformation that maps a 3D point from the camera reference frame to the world reference frame.

                                # Store the GT values for the edge, will be used later to compute loss.
                                y[edge_id, 0:7] = pose_algebra.compute_relative_pose_quaternion(torch.tensor(gt_dest_T, device='cpu'), torch.tensor(gt_source_T, device='cpu'), device='cpu')

                                edge_id += 1

                            elif type_of_input_estimates == 1:  # 1: Estimates from matching BB centres, not always available.
                                try:
                                    # When this fails, the edge is not created.
                                    edge_attr[edge_id, 0:7] =  pose_algebra.pose_matrix_to_pose_quaternion(torch.tensor(input_graph['relative_poses'][source_node_id][dest_node_id]['bb_centre_5pt'], device='cpu'), device='cpu')  # 1: Estimates from matching BB centres.  # 1: Estimates from matching BB centres.

                                    # Create the edge (add it to the data structure).
                                    edge_index[:, edge_id] = torch.tensor([source_node_index, dest_node_index])

                                    # Get the absolute camera poses.
                                    gt_source_T = input_graph['cameras'][str(source_node_id)]['camera_to_world']  # Extrinsics. This is the transformation that maps a 3D point from the camera reference frame to the world reference frame.
                                    gt_dest_T = input_graph['cameras'][str(dest_node_id)]['camera_to_world']  # Extrinsics. This is the transformation that maps a 3D point from the camera reference frame to the world reference frame.

                                    # Store the GT values for the edge, will be used later to compute loss.
                                    y[edge_id, 0:7] = pose_algebra.compute_relative_pose_quaternion(torch.tensor(gt_dest_T, device='cpu'), torch.tensor(gt_source_T, device='cpu'),device='cpu')
                                    edge_id += 1
                                except:
                                    # In case the estimate is not available, do not create the edge!
                                    pass

                            elif type_of_input_estimates == 2:  # 2: Estimates from matching keypoints, not always available.
                                try:
                                    # When this fails, the edge is not created.
                                    edge_attr[edge_id, 0:7]  =  pose_algebra.pose_matrix_to_pose_quaternion(torch.tensor(input_graph['relative_poses'][source_node_id][dest_node_id]['feature_5pt'], device='cpu'), device='cpu')  # 2: Estimates from matching keypoints.

                                    edge_index[:, edge_id] = torch.tensor([source_node_index, dest_node_index])

                                    # Get the absolute camera poses.
                                    gt_source_T = input_graph['cameras'][str(source_node_id)]['camera_to_world']  # Extrinsics. This is the transformation that maps a 3D point from the camera reference frame to the world reference frame.
                                    gt_dest_T = input_graph['cameras'][str(dest_node_id)]['camera_to_world']  # Extrinsics. This is the transformation that maps a 3D point from the camera reference frame to the world reference frame.

                                    # Store the GT values for the edge, will be used later to compute loss.
                                    y[edge_id, 0:7] = pose_algebra.compute_relative_pose_quaternion(torch.tensor(gt_dest_T, device='cpu'), torch.tensor(gt_source_T, device='cpu'),device='cpu')
                                    edge_id += 1
                                except:
                                    # In case the estimate is not available, do not create the edge!
                                    pass

                            elif type_of_input_estimates == 3:  # 3: Random poses, always available.
                                edge_attr[edge_id, 0:4] = generate_random_quaternion()
                                edge_attr[edge_id, 4:7] = generate_random_direction()

                                # Create the edge (add it to the data structure).
                                edge_index[:, edge_id] = torch.tensor([source_node_index, dest_node_index])

                                # Get the absolute camera poses.
                                gt_source_T = input_graph['cameras'][str(source_node_id)]['camera_to_world']  # Extrinsics. This is the transformation that maps a 3D point from the camera reference frame to the world reference frame.
                                gt_dest_T   = input_graph['cameras'][str(dest_node_id)]['camera_to_world']    # Extrinsics. This is the transformation that maps a 3D point from the camera reference frame to the world reference frame.

                                # Store the GT values for the edge, will be used later to compute loss.
                                y[edge_id, 0:7] = pose_algebra.compute_relative_pose_quaternion(torch.tensor(gt_dest_T, device='cpu'), torch.tensor(gt_source_T, device='cpu'), device='cpu')

                                edge_id += 1

                            elif type_of_input_estimates == 4:  # 4: Estimated from matching keypoints over the whole image.
                                try:
                                    # When this fails, the edge is not created.
                                    edge_attr[edge_id, 0:7] = pose_algebra.pose_matrix_to_pose_quaternion(torch.tensor(input_graph['relative_poses'][source_node_id][dest_node_id]['free_keypoint'], device='cpu'), device='cpu')  # 2: Estimates from matching keypoints.

                                    edge_index[:, edge_id] = torch.tensor([source_node_index, dest_node_index])

                                    # Get the absolute camera poses.
                                    gt_source_T = input_graph['cameras'][str(source_node_id)]['camera_to_world']  # Extrinsics. This is the transformation that maps a 3D point from the camera reference frame to the world reference frame.
                                    gt_dest_T = input_graph['cameras'][str(dest_node_id)]['camera_to_world']  # Extrinsics. This is the transformation that maps a 3D point from the camera reference frame to the world reference frame.

                                    # Store the GT values for the edge, will be used later to compute loss.
                                    y[edge_id, 0:7] = pose_algebra.compute_relative_pose_quaternion(torch.tensor(gt_dest_T, device='cpu'), torch.tensor(gt_source_T, device='cpu'), device='cpu')
                                    edge_id += 1
                                except:
                                    # In case the estimate is not available, do not create the edge!
                                    pass

            # Remove the unused parts of edge_attr, edge_index and y.
            n_edges = edge_id
            edge_attr             = edge_attr[0:n_edges, :]
            edge_index = edge_index[:, 0:n_edges]
            y = y[0:n_edges, :]
            n_edges = torch.tensor(edge_index.shape[1], device='cpu').reshape(1, 1)



            ###########################################################
            # 3. Create the indices that will be used to assemble the #
            # input for the MLP's when running the GNN.               #
            ###########################################################
            # 1. Compute the indices for updating the edges (only one direction).
            # n_max_contributions_per_edge = 40   # Maximum number of updates per edge: sum of the objects seen by both nodes
            #                                     # connected by one edge.
            #                                     # Should the nodes connected by one edge have more detected objects in
            #                                     # common, the extra ones would be ignored.

            temp_indices_EDGES = torch.zeros(0, 1, dtype=torch.long, device='cpu')
            first_det_indices_EDGES = torch.zeros(0, 1, dtype=torch.long, device='cpu')
            second_det_indices_EDGES = torch.zeros(0, 1, dtype=torch.long, device='cpu')
            det_id = 0
            image_ids = list(input_graph['cameras'].keys())
            for edge_id in range(edge_index.shape[1]):
                source_node_id = edge_index[0, edge_id]
                dest_node_id   = edge_index[1, edge_id]

                #  Given a pair of images (source node ID, dest node ID), find all pairs of matched detections.
                #  We need them in the shape of "source_det_ptr" and "dest_det_ptr".
                source_image = input_graph['cameras'][image_ids[source_node_id]]
                dest_image   = input_graph['cameras'][image_ids[dest_node_id]]
                matches = source_image['matches'][image_ids[dest_node_id]]  # These are the matches as seen from the source image to the destination image.
                for match in matches:
                    # Find the index of the column in x_det_ptr that corresponds to each detection in the pair.
                    source_det_index = source_image['detections'].index(match['bb_i'])
                    dest_det_index   = dest_image['detections'].index(match['bb_f'])
                    source_det_ptr = x_det_ptr[source_node_id][source_det_index]  # This is the index of the source detection in det_features.
                    dest_det_ptr   = x_det_ptr[dest_node_id][dest_det_index]      # This is the index of the destination detection in det_features.

                    # We found a match, we need to create one input row for the MLP with these data.
                    # edge_id is also the index of the rows of temp.
                    temp_indices_EDGES = torch.cat([temp_indices_EDGES, torch.tensor(edge_id, device='cpu').reshape(1, 1)], dim=0)
                    first_det_indices_EDGES = torch.cat([first_det_indices_EDGES, source_det_ptr.reshape(1, 1)], dim=0)
                    second_det_indices_EDGES = torch.cat([second_det_indices_EDGES, dest_det_ptr.reshape(1, 1)], dim=0)
                    det_id += 1
            n_matched_detections = det_id

            # 2. Replicate the edges, so that we have edges going in both directions.
            backward_edges = torch.zeros(edge_index.shape, dtype=torch.long, device='cpu')
            backward_edges[0, :] = edge_index[1, :]
            backward_edges[1, :] = edge_index[0, :]
            edge_index_NODES = torch.cat([edge_index, backward_edges], dim=-1)

            # 3. Compute indices for updating the nodes (need to consider both directions).
            # n_max_contributions_per_node = 400  # Maximum number of updates per node: sum of the objects seen by the current
            #                                     # node and at least by another node.
            #                                     # Say this node sees 2 objects, and one of those objects is seen in 5 other
            #                                     # images, while the other is not seen anywhere else, the number of
            #                                     # contributions for the current node is 10.
            #                                     # Should a node have more contributions, the extra ones would be ignored.
            temp_indices_NODES = torch.zeros(0, 1, dtype=torch.long, device='cpu')
            first_det_indices_NODES = torch.zeros(0, 1, dtype=torch.long, device='cpu')
            second_det_indices_NODES = torch.zeros(0, 1, dtype=torch.long, device='cpu')
            indices_for_aggregating_nodes_updates = torch.zeros(n_matched_detections * 2, dtype=torch.long, device='cpu')  # Used to update aggregate single updates into node updates. Works for both forward and backward edges, hence the x2.

            det_id = 0
            for edge_id in range(edge_index_NODES.shape[1]):
                source_node_id = edge_index_NODES[0, edge_id]
                dest_node_id   = edge_index_NODES[1, edge_id]

                #  Given a pair of images (source node ID, dest node ID), find all pairs of matched detections.
                #  We need them in the shape of "source_det_ptr" and "dest_det_ptr".
                source_image = input_graph['cameras'][image_ids[source_node_id]]
                dest_image   = input_graph['cameras'][image_ids[dest_node_id]]
                matches = source_image['matches'][image_ids[dest_node_id]]
                for match in matches:
                    # Find the index of the column in x_det_ptr that corresponds to each detection in the pair.
                    source_det_index = source_image['detections'].index(match['bb_i'])
                    dest_det_index   = dest_image['detections'].index(match['bb_f'])
                    source_det_ptr = x_det_ptr[source_node_id][source_det_index]
                    dest_det_ptr   = x_det_ptr[dest_node_id][dest_det_index]

                    # We found a match, we need to create one input row for the MLP with these data.
                    # edge_id is also the index of the rows of temp.
                    temp_indices_NODES = torch.cat([temp_indices_NODES, torch.tensor(edge_id, device='cpu').reshape(1, 1)], dim=0)
                    first_det_indices_NODES = torch.cat([first_det_indices_NODES, source_det_ptr.reshape(1, 1)], dim=0)
                    second_det_indices_NODES = torch.cat([second_det_indices_NODES, dest_det_ptr.reshape(1, 1)], dim=0)
                    indices_for_aggregating_nodes_updates[det_id] = dest_node_id  # The index contains the ID of the destination node, the one that is updated by single_updates[det_id].
                    det_id += 1

            if edge_attr.shape[0] == 0:
                print(graph_id)
            # This is the graph object (dictionary).
            data = MyGraph(x=x, y=y, edge_index=edge_index, edge_index_NODES=edge_index_NODES, edge_attr=edge_attr,  # Original attributes of a graph.
                           det_features=det_features, x_det_ptr=x_det_ptr, n_dets=n_dets,
                           temp_indices_NODES=temp_indices_NODES, temp_indices_EDGES=temp_indices_EDGES,
                           first_det_indices_NODES=first_det_indices_NODES, first_det_indices_EDGES=first_det_indices_EDGES,
                           second_det_indices_NODES=second_det_indices_NODES, second_det_indices_EDGES=second_det_indices_EDGES,
                           indices_for_aggregating_nodes_updates=indices_for_aggregating_nodes_updates,
                           n_edges=n_edges,
                           n_nodes=n_nodes,
                           graph_id=graph_id,
                           gt_absolute_poses=gt_absolute_poses)

            return data
        else:
            return []




def load_data_for_all_scenes_TRAIN(dataset_root_dir, sequence_name, save_data_to_binary_files, type_of_input_estimates):
    #################################################################################
    # Load data for one sequence of 7 Scenes while creating corrupted input values. #
    # Save a cached version of the data.                                            #
    #################################################################################
    if type_of_input_estimates == 0:
        type_of_input_label = 'corrupted'
    elif type_of_input_estimates == 1:
        type_of_input_label = 'bb'
    elif type_of_input_estimates == 2:
        type_of_input_label = 'keypoints'
    elif type_of_input_estimates == 3:
        type_of_input_label = 'random'
    elif type_of_input_estimates == 4:
        type_of_input_label = 'keypoints_over_whole_image'

    data_list = []  # A list of graphs.

    json_file_name = dataset_root_dir + '/' + sequence_name + '.json'
    print('* Loading data from JSON file: {:s}.'.format(json_file_name))
    with open(json_file_name, 'r') as infile:
        input_list = json.load(infile)

    load_data_in_parallel = True
    if load_data_in_parallel:
        n_cores = multiprocessing.cpu_count()
        data_list = Parallel(n_jobs=n_cores)(delayed(get_data_for_one_graph)(input_graph, graph_id, type_of_input_estimates) for graph_id, input_graph in enumerate(input_list['train']))
    else:
        for graph_id, input_graph in enumerate(input_list['train']):
            current_graph = get_data_for_one_graph(input_graph, graph_id)
            if current_graph.graph_id != -999:  # An empty list is returned if the graph is not well-formed.
                data_list.append(current_graph)
    print('* Filtering out empty graphs.')
    data_list = filter_out_empty_graphs(data_list)

    if save_data_to_binary_files:
        # Save a cached binary version of the data
        cached_data_dir = dataset_root_dir + '/cached_binary_input_data/'
        Path(cached_data_dir).mkdir(parents=True, exist_ok=True)
        binary_file_name = cached_data_dir + sequence_name + '_train_' + type_of_input_label + '.pkl'
        print('* Saving data to binary file: {:s}.'.format(binary_file_name))
        file_handle = open(binary_file_name, "wb")
        pickle.dump(data_list, file_handle)
        file_handle.close()

    return data_list

def load_data_for_all_scenes_leave_one_out_TRAIN(dataset_root_dir, sequence_name, save_data_to_binary_files,
                                                 type_of_input_estimates, left_out_scene_id):
    this_is_training = True
    #################################################################################
    # Load data for one sequence of 7 Scenes while creating corrupted input values. #
    # Save a cached version of the data.                                            #
    #################################################################################
    if type_of_input_estimates == 0:
        type_of_input_label = 'corrupted'
    elif type_of_input_estimates == 1:
        type_of_input_label = 'bb'
    elif type_of_input_estimates == 2:
        type_of_input_label = 'keypoints'
    elif type_of_input_estimates == 3:
        type_of_input_label = 'random'
    elif type_of_input_estimates == 4:
        type_of_input_label = 'keypoints_over_whole_image'

    data_list = []  # A list of graphs.

    json_file_name = dataset_root_dir + '/' + sequence_name + '.json'
    print('* Loading data from JSON file: {:s}.'.format(json_file_name))
    with open(json_file_name, 'r') as infile:
        input_list = json.load(infile)

    load_data_in_parallel = True
    if load_data_in_parallel:
        n_cores = multiprocessing.cpu_count()
        data_list = Parallel(n_jobs=n_cores)(delayed(get_data_for_one_graph)(input_graph, graph_id, type_of_input_estimates,
                                                                             this_is_training=this_is_training, left_out_scene_id=left_out_scene_id) for graph_id, input_graph in enumerate(input_list['train']))
    else:
        for graph_id, input_graph in enumerate(input_list['train']):
            current_graph = get_data_for_one_graph(input_graph, graph_id, type_of_input_estimates,
                                                   this_is_training=this_is_training, left_out_scene_id=left_out_scene_id)
            if current_graph.graph_id != -999:  # An empty list is returned if the graph is not well-formed.
                data_list.append(current_graph)
    print('* Filtering out empty graphs.')
    data_list = filter_out_empty_graphs(data_list)

    if save_data_to_binary_files:
        # Save a cached binary version of the data
        cached_data_dir = dataset_root_dir + '/cached_binary_input_data/'
        Path(cached_data_dir).mkdir(parents=True, exist_ok=True)
        binary_file_name = cached_data_dir + sequence_name + '_train_' + type_of_input_label + '_loo_{:d}.pkl'.format(left_out_scene_id)

        print('* Saving data to binary file: {:s}.'.format(binary_file_name))
        file_handle = open(binary_file_name, "wb")
        pickle.dump(data_list, file_handle)
        file_handle.close()

    return data_list


def load_data_from_for_all_scenes_leave_one_out_VALIDATION(dataset_root_dir, sequence_name, save_data_to_binary_files, type_of_input_estimates, left_out_scene_id):
    this_is_training = False

    #################################################################################
    # Load data for one sequence of 7 Scenes while creating corrupted input values. #
    # Save a cached version of the data.                                            #
    #################################################################################
    if type_of_input_estimates == 0:
        type_of_input_label = 'corrupted'
    elif type_of_input_estimates == 1:
        type_of_input_label = 'bb'
    elif type_of_input_estimates == 2:
        type_of_input_label = 'keypoints'
    elif type_of_input_estimates == 3:
        type_of_input_label = 'random'
    elif type_of_input_estimates == 4:
        type_of_input_label = 'keypoints_over_whole_image'

    data_list = []  # A list of graphs.

    json_file_name = dataset_root_dir + '/' + sequence_name + '.json'
    print('* Loading data from JSON file: {:s}.'.format(json_file_name))
    with open(json_file_name, 'r') as infile:
        input_list = json.load(infile)

    load_data_in_parallel = True
    if load_data_in_parallel:
        n_cores = multiprocessing.cpu_count()
        data_list = Parallel(n_jobs=n_cores)(delayed(get_data_for_one_graph)(input_graph, graph_id, type_of_input_estimates,
                                                                             this_is_training=this_is_training, left_out_scene_id=left_out_scene_id) for graph_id, input_graph in enumerate(input_list['test']))
    else:
        for graph_id, input_graph in enumerate(input_list['test']):
            current_graph = get_data_for_one_graph(input_graph, graph_id, type_of_input_estimates,
                                                   this_is_training=this_is_training, left_out_scene_id=left_out_scene_id)
            if current_graph.graph_id != -999:  # An empty list is returned if the graph is not well-formed.
                data_list.append(current_graph)
    print('* Filtering out empty graphs.')
    data_list = filter_out_empty_graphs(data_list)

    if save_data_to_binary_files:
        # Save a cached binary version of the data
        cached_data_dir = dataset_root_dir + '/cached_binary_input_data/'
        Path(cached_data_dir).mkdir(parents=True, exist_ok=True)
        binary_file_name = cached_data_dir + sequence_name + '_validation_' + type_of_input_label + '_loo_{:d}.pkl'.format(left_out_scene_id)

        print('* Saving data to binary file: {:s}.'.format(binary_file_name))
        file_handle = open(binary_file_name, "wb")
        pickle.dump(data_list, file_handle)
        file_handle.close()

    return data_list



def load_data_from_for_all_scenes_VALIDATION(dataset_root_dir, sequence_name, save_data_to_binary_files, type_of_input_estimates):
    #################################################################################
    # Load data for one sequence of 7 Scenes while creating corrupted input values. #
    # Save a cached version of the data.                                            #
    #################################################################################
    if type_of_input_estimates == 0:
        type_of_input_label = 'corrupted'
    elif type_of_input_estimates == 1:
        type_of_input_label = 'bb'
    elif type_of_input_estimates == 2:
        type_of_input_label = 'keypoints'
    elif type_of_input_estimates == 3:
        type_of_input_label = 'random'
    elif type_of_input_estimates == 4:
        type_of_input_label = 'keypoints_over_whole_image'

    data_list = []  # A list of graphs.

    json_file_name = dataset_root_dir + '/' + sequence_name + '.json'
    print('* Loading data from JSON file: {:s}.'.format(json_file_name))
    with open(json_file_name, 'r') as infile:
        input_list = json.load(infile)

    load_data_in_parallel = True
    if load_data_in_parallel:
        n_cores = multiprocessing.cpu_count()
        data_list = Parallel(n_jobs=n_cores)(delayed(get_data_for_one_graph)(input_graph, graph_id, type_of_input_estimates) for graph_id, input_graph in enumerate(input_list['test']))
    else:
        for graph_id, input_graph in enumerate(input_list['test']):
            current_graph = get_data_for_one_graph(input_graph, graph_id)
            if current_graph.graph_id != -999:  # An empty list is returned if the graph is not well-formed.
                data_list.append(current_graph)
    print('* Filtering out empty graphs.')
    data_list = filter_out_empty_graphs(data_list)

    if save_data_to_binary_files:
        # Save a cached binary version of the data
        cached_data_dir = dataset_root_dir + '/cached_binary_input_data/'
        Path(cached_data_dir).mkdir(parents=True, exist_ok=True)
        binary_file_name = cached_data_dir + sequence_name + '_validation_' + type_of_input_label + '.pkl'

        print('* Saving data to binary file: {:s}.'.format(binary_file_name))
        file_handle = open(binary_file_name, "wb")
        pickle.dump(data_list, file_handle)
        file_handle.close()

    return data_list



def get_training_data_for_all_scenes(dataset_root_dir, sequence_name, load_data_from_binary_files=False, save_data_to_binary_files=False, type_of_input_estimates=0):
    """ Returns a set of graphs corresponding to one sequence of the 7 Scenes dataset.

        Each graph contains several extra data structures respect to standard PyG graphs, including:
        * detections: the list of all the detections associated with the graph.
        * x_det_ptr: the list of indices that associates detections to the node they refer to.
                     A node corresponds to one image, so detections are associated to the image they refer to.

        Returns a list of graphs.
    """
    if load_data_from_binary_files:
        if type_of_input_estimates == 0:
            type_of_input_label = 'corrupted'
        elif type_of_input_estimates == 1:
            type_of_input_label = 'bb'
        elif type_of_input_estimates == 2:
            type_of_input_label = 'keypoints'
        elif type_of_input_estimates == 3:
            type_of_input_label = 'random'
        elif type_of_input_estimates == 4:
            type_of_input_label = 'keypoints_over_whole_image'

        cached_data_dir = dataset_root_dir + '/cached_binary_input_data/'

        file_name = cached_data_dir + sequence_name + '_train_' + type_of_input_label + '.pkl'
        print('* Loading data from binary file: {:s}.'.format(file_name))
        file_handle = open(file_name, 'rb')
        data_list = pickle.load(file_handle)
        print('* Filtering out empty graphs.')
        data_list = filter_out_empty_graphs(data_list)

    else:
        data_list = load_data_for_all_scenes_TRAIN(dataset_root_dir, sequence_name, save_data_to_binary_files, type_of_input_estimates)

    return data_list



def get_training_data_for_all_scenes_leave_one_out(dataset_root_dir, sequence_name, load_data_from_binary_files=False,
                                                   save_data_to_binary_files=False, type_of_input_estimates=0, left_out_scene_id=0):
    """ Returns a set of graphs corresponding to one sequence of the 7 Scenes dataset.

        Each graph contains several extra data structures respect to standard PyG graphs, including:
        * detections: the list of all the detections associated with the graph.
        * x_det_ptr: the list of indices that associates detections to the node they refer to.
                     A node corresponds to one image, so detections are associated to the image they refer to.

        Returns a list of graphs.
    """
    if load_data_from_binary_files:
        if type_of_input_estimates == 0:
            type_of_input_label = 'corrupted'
        elif type_of_input_estimates == 1:
            type_of_input_label = 'bb'
        elif type_of_input_estimates == 2:
            type_of_input_label = 'keypoints'
        elif type_of_input_estimates == 3:
            type_of_input_label = 'random'
        elif type_of_input_estimates == 4:
            type_of_input_label = 'keypoints_over_whole_image'

        cached_data_dir = dataset_root_dir + '/cached_binary_input_data/'
        file_name = cached_data_dir + sequence_name + '_train_' + type_of_input_label + '_loo_{:d}.pkl'.format(left_out_scene_id)
        print('* Loading data from binary file: {:s}.'.format(file_name))
        file_handle = open(file_name, 'rb')
        data_list = pickle.load(file_handle)
        print('* Filtering out empty graphs.')
        data_list = filter_out_empty_graphs(data_list)

    else:
        data_list = load_data_for_all_scenes_leave_one_out_TRAIN(dataset_root_dir, sequence_name, save_data_to_binary_files, type_of_input_estimates, left_out_scene_id)

    return data_list



def get_validation_data_for_all_scenes(dataset_root_dir, sequence_name, load_data_from_binary_files=False, save_data_to_binary_files=False, type_of_input_estimates=0):
    """ Returns a set of graphs corresponding to one sequence of the 7 Scenes dataset.

        Each graph contains several extra data structures respect to standard PyG graphs, including:
        * detections: the list of all the detections associated with the graph.
        * x_det_ptr: the list of indices that associates detections to the node they refer to.
                     A node corresponds to one image, so detections are associated to the image they refer to.

        Returns a list of graphs.
    """
    if load_data_from_binary_files:
        if type_of_input_estimates == 0:
            type_of_input_label = 'corrupted'
        elif type_of_input_estimates == 1:
            type_of_input_label = 'bb'
        elif type_of_input_estimates == 2:
            type_of_input_label = 'keypoints'
        elif type_of_input_estimates == 3:
            type_of_input_label = 'random'
        elif type_of_input_estimates == 4:
            type_of_input_label = 'keypoints_over_whole_image'

        cached_data_dir = dataset_root_dir + '/cached_binary_input_data/'


        file_name = cached_data_dir + sequence_name + '_validation_' + type_of_input_label + '.pkl'
        print('* Loading data from binary file: {:s}.'.format(file_name))
        file_handle = open(file_name, 'rb')
        data_list = pickle.load(file_handle)
        print('* Filtering out empty graphs.')
        data_list = filter_out_empty_graphs(data_list)

    else:
        data_list = load_data_from_for_all_scenes_VALIDATION(dataset_root_dir, sequence_name, save_data_to_binary_files, type_of_input_estimates)

    return data_list



def get_validation_data_for_all_scenes_leave_one_out(dataset_root_dir, sequence_name, load_data_from_binary_files=False,
                                                     save_data_to_binary_files=False, type_of_input_estimates=0, left_out_scene_id=0):
    """ Returns a set of graphs corresponding to one sequence of the 7 Scenes dataset.

        Each graph contains several extra data structures respect to standard PyG graphs, including:
        * detections: the list of all the detections associated with the graph.
        * x_det_ptr: the list of indices that associates detections to the node they refer to.
                     A node corresponds to one image, so detections are associated to the image they refer to.

        Returns a list of graphs.
    """
    if load_data_from_binary_files:
        if type_of_input_estimates == 0:
            type_of_input_label = 'corrupted'
        elif type_of_input_estimates == 1:
            type_of_input_label = 'bb'
        elif type_of_input_estimates == 2:
            type_of_input_label = 'keypoints'
        elif type_of_input_estimates == 3:
            type_of_input_label = 'random'
        elif type_of_input_estimates == 4:
            type_of_input_label = 'keypoints_over_whole_image'

        cached_data_dir = dataset_root_dir + '/cached_binary_input_data/'
        file_name = cached_data_dir + sequence_name + '_validation_' + type_of_input_label + '_loo_{:d}.pkl'.format(left_out_scene_id)
        print('* Loading data from binary file: {:s}.'.format(file_name))
        file_handle = open(file_name, 'rb')
        data_list = pickle.load(file_handle)
        print('* Filtering out empty graphs.')
        data_list = filter_out_empty_graphs(data_list)

    else:
        data_list = load_data_from_for_all_scenes_leave_one_out_VALIDATION(dataset_root_dir, sequence_name, save_data_to_binary_files, type_of_input_estimates, left_out_scene_id)

    return data_list



def filter_out_empty_graphs(graph_list):
    n_kept_graphs = 0
    n_empty_graphs = 0
    n_filtered_out_graphs = 0
    output_list = []
    for graph in graph_list:
        if len(graph) > 0:
            if graph.edge_attr.shape[0] > 0:
                output_list.append(graph)
                n_kept_graphs += 1
            else:
                print('I filtered out one graph because it was empty.')
                n_empty_graphs += 1
        else:
            # print('I filtered out one graph because it was from the specified sequence.')
            n_filtered_out_graphs += 1
    print('* Results of filtering: {:d} graphs were kept, {:d} graphs were filtered out because they belong to the specified sequence, {:d} graphs were removed because they were empty.'.
          format(n_kept_graphs, n_filtered_out_graphs, n_empty_graphs))
    return output_list
